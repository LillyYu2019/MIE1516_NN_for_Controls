{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import pickle\n",
    "import time\n",
    "import sys\n",
    "import math\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning: Table form Q-learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper Parameters\n",
    "- **nbins** is number of discretization you can do, currently maximum of 98\n",
    "- **GAMMA** is the discount factor\n",
    "-  **ALPHA** is the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbins = 10\n",
    "GAMMA = 0.9\n",
    "ALPHA = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define some functions for running the tabular Q learning with greedy epsilon strategy.\n",
    "\n",
    "*Reference: * Phil Tabor, OpenAI-Cartpole, (2017), GitHub repository, https://github.com/philtabor/OpenAI-Cartpole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_dict(d): \n",
    "    \"\"\"\n",
    "    looking for the action that gives the maximum value for a given state\n",
    "    \"\"\"\n",
    "    max_v = float('-inf')\n",
    "    for key, val in d.items():\n",
    "        if val > max_v:\n",
    "            max_v = val\n",
    "            max_key = key\n",
    "    return max_key, max_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bins(): \n",
    "    \"\"\"\n",
    "    create bins to discretize the continuous observable state space\n",
    "    \"\"\"\n",
    "    # obs[0] -> cart position --- -4.8 - 4.8\n",
    "    # obs[1] -> cart velocity --- -inf - inf\n",
    "    # obs[2] -> pole angle    --- -41.8 - 41.8\n",
    "    # obs[3] -> pole velocity --- -inf - inf\n",
    "\n",
    "    bins = np.zeros((4,nbins))\n",
    "    bins[0] = np.linspace(-4.8, 4.8, nbins)\n",
    "    bins[1] = np.linspace(-5, 5, nbins)\n",
    "    bins[2] = np.linspace(-.418, .418, nbins)\n",
    "    bins[3] = np.linspace(-5, 5, nbins)\n",
    "    return bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_bins(observation, bins): \n",
    "    \"\"\"\n",
    "    discretizing the continuous observation space into state\n",
    "    \"\"\"\n",
    "    state = np.zeros(4)\n",
    "    for i in range(4):\n",
    "        state[i] = np.digitize(observation[i], bins[i])\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state_as_string(state):\n",
    "    \"\"\"\n",
    "    encoding the state into string as dictionary\n",
    "    \"\"\"\n",
    "    string_state=''\n",
    "    for e in state:\n",
    "            string_state = string_state+str(int(e)).zfill(2)\n",
    "    return string_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_states_as_string():\n",
    "    states = []\n",
    "    for i in range (nbins+1):\n",
    "        for j in range (nbins+1):\n",
    "            for k in range(nbins+1):\n",
    "                for l in range(nbins+1):\n",
    "                    a=str(i).zfill(2)+str(j).zfill(2)+str(k).zfill(2)+str(l).zfill(2)\n",
    "                    states.append(a)\n",
    "    return states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_Q():\n",
    "    \"\"\"\n",
    "    initialize your Q table\n",
    "    \"\"\"\n",
    "    Q = {}\n",
    "\n",
    "    all_states = get_all_states_as_string()\n",
    "    for state in all_states:\n",
    "        Q[state] = {}\n",
    "        for action in range(env.action_space.n):\n",
    "            Q[state][action] = 0\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one_game(bins, Q, eps=0.5):\n",
    "    \"\"\"\n",
    "    train 1 episode\n",
    "    \"\"\"\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    cnt = 0 # number of moves in an episode\n",
    "    state = get_state_as_string(assign_bins(observation, bins))\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        cnt += 1\n",
    "        # np.random.randn() seems to yield a random action 50% of the time ?\n",
    "        if np.random.uniform() < eps:\n",
    "            act = env.action_space.sample() # epsilon greedy\n",
    "        else:\n",
    "            act = max_dict(Q[state])[0]\n",
    "\n",
    "        observation, reward, done, _ = env.step(act)\n",
    "\n",
    "        total_reward += reward\n",
    "\n",
    "        if done and cnt < 200:\n",
    "            reward = -300\n",
    "\n",
    "        state_new = get_state_as_string(assign_bins(observation, bins))\n",
    "\n",
    "        a1, max_q_s1a1 = max_dict(Q[state_new])\n",
    "        Q[state][act] += ALPHA*(reward + GAMMA*max_q_s1a1 - Q[state][act])\n",
    "        state, act = state_new, a1\n",
    "\n",
    "    return total_reward, cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_many_games(bins, N=10000):\n",
    "    \"\"\"\n",
    "    train many episodes\n",
    "    \"\"\"\n",
    "    Q = initialize_Q()\n",
    "\n",
    "    length = []\n",
    "    reward = []\n",
    "    for n in range(N):\n",
    "        #eps=0.5/(1+n*10e-3)\n",
    "        eps = 1.0 / np.sqrt(n+1)\n",
    "\n",
    "        episode_reward, episode_length= play_one_game(bins, Q, eps)\n",
    "\n",
    "        if n % 100 == 0:\n",
    "            clear_output(wait=True)\n",
    "            print(\"Episode: %d, Epislon: %.4f, Reward %d\"%(n,eps,episode_reward))\n",
    "        length.append(episode_length)\n",
    "        reward.append(episode_reward)\n",
    "    env.close()\n",
    "    return length, reward, Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_running_avg(totalrewards,title='Running Average',save=False,name='result'):\n",
    "    \"\"\"\n",
    "    plotting the average reward during training\n",
    "    \"\"\"\n",
    "    fig=plt.figure()\n",
    "    N = len(totalrewards)\n",
    "    running_avg = np.empty(N)\n",
    "    for t in range(N):\n",
    "        running_avg[t] = np.mean(totalrewards[max(0, t-100):(t+1)])\n",
    "    plt.plot(running_avg)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Duration\")\n",
    "#     plt.grid()\n",
    "    if save:\n",
    "        plt.savefig(name+'.png',bbox_inches='tight')\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_policy(bins,Q,N=1000,render=False,delay=0.01):\n",
    "    \"\"\"\n",
    "    run an environment using a trained policy\n",
    "    \"\"\"\n",
    "    \n",
    "    totalReward=[]\n",
    "    steps=[]\n",
    "    for n in range(N):\n",
    "        observation=env.reset()\n",
    "        done=False\n",
    "        episodeReward=0\n",
    "        while not done:\n",
    "            if render:\n",
    "                env.render()\n",
    "                time.sleep(delay)\n",
    "            state=get_state_as_string(assign_bins(observation, bins))\n",
    "            act=max_dict(Q[state])[0]\n",
    "            observation,reward,done,_=env.step(act)\n",
    "            episodeReward+=reward\n",
    "        totalReward.append(episodeReward)\n",
    "    env.close()\n",
    "    return totalReward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Expert Policy\n",
    "\n",
    "Here we train a tradition Q-learning to use as the expert policy that we want the apprenticeship IRL algorithm to learn from. The reward obtained from each state-action pair is stored in a 10000 x 2 array, where the rows are the states and columns are the actions. Each cell is a reward associated with the state-action pair. The algorithm uses epislon-greedy approach choose its actions, and updates its Q table through the training iterations. This Q table is the policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 19900, Epislon: 0.0071, Reward 200\n",
      "export trained expert model...\n"
     ]
    }
   ],
   "source": [
    "bins = create_bins()\n",
    "episode_lengths, episode_rewards, expert_Q=play_many_games(bins,N=20000)\n",
    "\n",
    "print(\"export trained expert model...\")\n",
    "filename = 'expert_Q'\n",
    "outfile = open(filename,'wb')\n",
    "pickle.dump(expert_Q,outfile)\n",
    "outfile.close()\n",
    "\n",
    "#plot_running_avg(episode_rewards,title=\"Performance: Expert\",save=True,name='Expert')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstrate Expert Policy\n",
    "\n",
    "Here we show the training result. Using the trained policy, we run the environment 10000 times to sample the reward distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "expertReward=play_policy(bins,expert_Q,N=10000,render=False)\n",
    "\n",
    "plt.hist(expertReward,bins=50)\n",
    "plt.title(\"Reward Distribution\")\n",
    "plt.xlabel(\"Reward\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "unique, counts = np.unique(expertReward, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We render the environment to visualize the policy in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "expertReward=play_policy(bins,expert_Q,N=1,render=True,delay=0.05)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
